{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlC6Vk8Czjc6s7Xas9f60m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grace0607/OAI-API-Testing/blob/main/OAI_API_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI API Testing\n",
        "This notebook makes API calls to allow users to have multi-turn conversations with OpenAI models. Refer to https://platform.openai.com/docs/models for the full list of models and their specs.\n",
        "\n",
        "You can click 'Run all' in the Runtime tab to get started or 'Cmd+F9' as a shortcut.\n",
        "\n",
        "Last updated: 1/21/2025"
      ],
      "metadata": {
        "id": "RmMYBFWhiBfx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Install packages and configure API key**\n",
        "  Run this cell to install the required packages and to initialize the OpenAI client using your API key. Save your API key as an environment variable to the Colab \"Secrets\" ('Name' field should be a sort of nickname for your API key of your choice and 'Value' should be the actual value of your API key)and allow \"Notebook access.\"\n",
        "  \n",
        "  You only need to run this cell once."
      ],
      "metadata": {
        "id": "SQ8jYip2HhS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install openai\n",
        "\n",
        "import openai\n",
        "from datetime import datetime\n",
        "import json\n",
        "from google.colab import userdata\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "client = openai.OpenAI(\n",
        "    api_key=userdata.get('openai')\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEw9_RH7CHpn",
        "outputId": "9118ee80-f2fe-4efc-bf7e-d2f5fe5d1fb2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.59.6)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Create functions**\n",
        "  Run this cell to define all the API models and functions we need to test the API. You shouldn't see anything happen if the run is successful. It's a silent success!"
      ],
      "metadata": {
        "id": "V7onDwmBIqWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define available models with their specifications\n",
        "MODELS = {\n",
        "    # GPT-4o Series\n",
        "    \"gpt-4o\": {\"context_window\": 128000, \"max_output\": 16384, \"description\": \"Same as gpt-4o-2024-08-06\"},\n",
        "    \"gpt-4o-2024-08-06\": {\"context_window\": 128000, \"max_output\": 16384},\n",
        "    \"gpt-4o-2024-11-20\": {\"context_window\": 128000, \"max_output\": 16384},\n",
        "    \"gpt-4o-2024-05-13\": {\"context_window\": 128000, \"max_output\": 4096},\n",
        "    \"chatgpt-4o-latest\": {\"context_window\": 128000, \"max_output\": 16384, \"description\": \"GPT-4o used in ChatGPT UI\"},\n",
        "\n",
        "    # GPT-4o Mini Series\n",
        "    \"gpt-4o-mini-2024-07-18\": {\"context_window\": 128000, \"max_output\": 16384},\n",
        "\n",
        "    # o1 Series\n",
        "    \"o1-2024-12-17\": {\"context_window\": 200000, \"max_output\": 100000, \"description\": \"Same as o1\"},\n",
        "    \"o1-mini-2024-09-12\": {\"context_window\": 128000, \"max_output\": 65536},\n",
        "    \"o1-preview-2024-09-12\": {\"context_window\": 128000, \"max_output\": 32768},\n",
        "\n",
        "    # DALL-E Series\n",
        "    \"dall-e-3\": {\"description\": \"Latest DALL·E model released in Nov 2023\"},\n",
        "    \"dall-e-2\": {\"description\": \"Previous DALL·E model with 4x greater resolution than original\"},\n",
        "\n",
        "    # GPT-4o Realtime Preview\n",
        "    \"gpt-4o-realtime-preview-2024-12-17\": {\"context_window\": 128000, \"max_output\": 4096},\n",
        "    \"gpt-4o-realtime-preview-2024-10-01\": {\"context_window\": 128000, \"max_output\": 4096},\n",
        "    \"gpt-4o-mini-realtime-preview-2024-12-17\": {\"context_window\": 128000, \"max_output\": 4096},\n",
        "\n",
        "    # GPT-4o Audio Preview\n",
        "    \"gpt-4o-audio-preview-2024-12-17\": {\"context_window\": 128000, \"max_output\": 16384},\n",
        "    \"gpt-4o-audio-preview-2024-10-01\": {\"context_window\": 128000, \"max_output\": 16384},\n",
        "    \"gpt-4o-mini-audio-preview-2024-12-17\": {\"context_window\": 128000, \"max_output\": 16384},\n",
        "\n",
        "    # GPT-4 Turbo and GPT-4\n",
        "    \"gpt-4-turbo-2024-04-09\": {\"context_window\": 128000, \"max_output\": 4096, \"description\": \"Same as gpt-4-turbo\"},\n",
        "    \"gpt-4-0125-preview\": {\"context_window\": 128000, \"max_output\": 4096},\n",
        "    \"gpt-4-1106-preview\": {\"context_window\": 128000, \"max_output\": 4096},\n",
        "    \"gpt-4-0613\": {\"context_window\": 8192, \"max_output\": 8192},\n",
        "    \"gpt-4-0314\": {\"context_window\": 8192, \"max_output\": 8192},\n",
        "\n",
        "    # GPT-3.5 Turbo\n",
        "    \"gpt-3.5-turbo-0125\": {\"context_window\": 16385, \"max_output\": 4096, \"description\": \"Same as gpt-3.5-turbo\"},\n",
        "    \"gpt-3.5-turbo-1106\": {\"context_window\": 16385, \"max_output\": 4096},\n",
        "    \"gpt-3.5-turbo-instruct\": {\"context_window\": 4096, \"max_output\": 4096},\n",
        "}\n",
        "\n",
        "def initialize_chat(model=\"gpt-4o\"):\n",
        "    \"\"\"Initialize a new chat session with specified model.\"\"\"\n",
        "    return {\n",
        "        \"model\": model,\n",
        "        \"messages\": [],\n",
        "        \"start_time\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    }\n",
        "\n",
        "def send_message(chat_session, user_message):\n",
        "    \"\"\"Send a message to the API and return the response.\"\"\"\n",
        "    # Add user message to history\n",
        "    chat_session[\"messages\"].append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "    try:\n",
        "        # Send to API\n",
        "        response = client.chat.completions.create(\n",
        "            model=chat_session[\"model\"],\n",
        "            messages=chat_session[\"messages\"],\n",
        "            temperature=1.0\n",
        "        )\n",
        "\n",
        "        # Get assistant's response\n",
        "        assistant_message = response.choices[0].message.content\n",
        "\n",
        "        # Add to history\n",
        "        chat_session[\"messages\"].append({\"role\": \"assistant\", \"content\": assistant_message})\n",
        "\n",
        "        return assistant_message\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def print_transcript(chat_session):\n",
        "    \"\"\"Print the full conversation transcript.\"\"\"\n",
        "    print(f\"\\nChat Transcript - Started at {chat_session['start_time']}\")\n",
        "    print(f\"Model: {chat_session['model']}\")\n",
        "    model_info = MODELS[chat_session['model']]\n",
        "    if 'context_window' in model_info:\n",
        "        print(f\"Context Window: {model_info['context_window']} tokens\")\n",
        "        print(f\"Max Output: {model_info['max_output']} tokens\")\n",
        "    if 'description' in model_info:\n",
        "        print(f\"Description: {model_info['description']}\")\n",
        "    print(\"\\n\" + \"-\" * 80)\n",
        "\n",
        "    for msg in chat_session[\"messages\"]:\n",
        "        role = msg[\"role\"].upper()\n",
        "        content = msg[\"content\"]\n",
        "        print(f\"\\n{role}:\")\n",
        "        print(content)\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "def export_transcript(chat_session, filename=None):\n",
        "    \"\"\"Export the conversation transcript to a JSON file.\"\"\"\n",
        "    if filename is None:\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"chat_transcript_{timestamp}.json\"\n",
        "\n",
        "    transcript_data = {\n",
        "        \"timestamp\": chat_session[\"start_time\"],\n",
        "        \"model\": chat_session[\"model\"],\n",
        "        \"model_specs\": MODELS[chat_session[\"model\"]],\n",
        "        \"messages\": chat_session[\"messages\"]\n",
        "    }\n",
        "\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(transcript_data, f, indent=2)\n",
        "\n",
        "    print(f\"\\nTranscript exported to {filename}\")\n",
        "\n",
        "def display_model_categories():\n",
        "    \"\"\"Display available model categories for selection.\"\"\"\n",
        "    categories = {\n",
        "        \"1\": \"GPT-4o Series\",\n",
        "        \"2\": \"GPT-4o Mini Series\",\n",
        "        \"3\": \"o1 Series\",\n",
        "        \"4\": \"DALL-E Series\",\n",
        "        \"5\": \"GPT-4o Realtime Preview\",\n",
        "        \"6\": \"GPT-4o Audio Preview\",\n",
        "        \"7\": \"GPT-4 Turbo and GPT-4\",\n",
        "        \"8\": \"GPT-3.5 Turbo\"\n",
        "    }\n",
        "\n",
        "    print(\"\\nAvailable Model Categories:\")\n",
        "    for num, category in categories.items():\n",
        "        print(f\"{num}. {category}\")\n",
        "\n",
        "    return categories\n",
        "\n",
        "def display_models_in_category(category):\n",
        "    \"\"\"Display models within a selected category.\"\"\"\n",
        "    model_filters = {\n",
        "        \"1\": {\"prefix\": \"gpt-4o\", \"include\": [\"chatgpt-4o\"]},\n",
        "        \"2\": {\"prefix\": \"gpt-4o-mini\"},\n",
        "        \"3\": {\"prefix\": \"o1\"},\n",
        "        \"4\": {\"prefix\": \"dall-e\"},\n",
        "        \"5\": {\"prefix\": \"gpt-4o-realtime\"},\n",
        "        \"6\": {\"prefix\": \"gpt-4o-audio\"},\n",
        "        \"7\": {\"prefix\": \"gpt-4\"},\n",
        "        \"8\": {\"prefix\": \"gpt-3.5\"}\n",
        "    }\n",
        "\n",
        "    filter_info = model_filters[category]\n",
        "    filtered_models = []\n",
        "\n",
        "    # Filter models based on prefix and additional includes\n",
        "    for model in MODELS.keys():\n",
        "        if model.startswith(filter_info[\"prefix\"]) or \\\n",
        "           (\"include\" in filter_info and any(inc in model for inc in filter_info[\"include\"])):\n",
        "            filtered_models.append(model)\n",
        "\n",
        "    print(\"\\nAvailable Models in Category:\")\n",
        "    for i, model in enumerate(filtered_models, 1):\n",
        "        specs = MODELS[model]\n",
        "        print(f\"{i}. {model}\")\n",
        "        if \"description\" in specs:\n",
        "            print(f\"   Description: {specs['description']}\")\n",
        "\n",
        "    return filtered_models\n",
        "\n",
        "def handle_dalle_request(prompt, model=\"dall-e-3\", size=\"1024x1024\", quality=\"standard\", n=1):\n",
        "    \"\"\"Handle DALL-E image generation requests.\"\"\"\n",
        "    try:\n",
        "        response = client.images.generate(\n",
        "            model=model,\n",
        "            prompt=prompt,\n",
        "            size=size,\n",
        "            quality=quality,\n",
        "            n=n,\n",
        "        )\n",
        "        return {\n",
        "            \"urls\": [item.url for item in response.data],\n",
        "            \"revised_prompt\": response.data[0].revised_prompt if hasattr(response.data[0], 'revised_prompt') else None\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating image: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def send_message(chat_session, user_message):\n",
        "    \"\"\"Send a message to the API and return the response.\"\"\"\n",
        "    # Check if it's a DALL-E model\n",
        "    if chat_session[\"model\"].startswith(\"dall-e\"):\n",
        "        result = handle_dalle_request(user_message, model=chat_session[\"model\"])\n",
        "        if result:\n",
        "            response_text = \"Image generated successfully!\\n\"\n",
        "            if result[\"revised_prompt\"]:\n",
        "                response_text += f\"\\nRevised prompt: {result['revised_prompt']}\\n\"\n",
        "            response_text += \"\\nImage URLs:\\n\" + \"\\n\".join(result[\"urls\"])\n",
        "        else:\n",
        "            response_text = \"Failed to generate image.\"\n",
        "\n",
        "        # Add to conversation history\n",
        "        chat_session[\"messages\"].append({\"role\": \"user\", \"content\": user_message})\n",
        "        chat_session[\"messages\"].append({\"role\": \"assistant\", \"content\": response_text})\n",
        "        return response_text\n",
        "\n",
        "    # Regular chat models\n",
        "    else:\n",
        "        chat_session[\"messages\"].append({\"role\": \"user\", \"content\": user_message})\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=chat_session[\"model\"],\n",
        "                messages=chat_session[\"messages\"],\n",
        "                temperature=1.0\n",
        "            )\n",
        "            assistant_message = response.choices[0].message.content\n",
        "            chat_session[\"messages\"].append({\"role\": \"assistant\", \"content\": assistant_message})\n",
        "            return assistant_message\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "def initialize_chat(model=\"gpt-4o\"):\n",
        "    \"\"\"Initialize a new chat session with specified model.\"\"\"\n",
        "    return {\n",
        "        \"model\": model,\n",
        "        \"messages\": [],\n",
        "        \"start_time\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    }\n",
        "\n",
        "def start_interactive_chat():\n",
        "    # Let user choose model category\n",
        "    categories = display_model_categories()\n",
        "    while True:\n",
        "        category_choice = input(\"\\nChoose category (number): \").strip()\n",
        "        if category_choice in categories:\n",
        "            break\n",
        "        print(\"Invalid category. Please try again.\")\n",
        "\n",
        "    # Let user choose specific model\n",
        "    filtered_models = display_models_in_category(category_choice)\n",
        "    while True:\n",
        "        model_num = input(\"\\nChoose model number: \").strip()\n",
        "        try:\n",
        "            model_index = int(model_num) - 1\n",
        "            if 0 <= model_index < len(filtered_models):\n",
        "                model = filtered_models[model_index]\n",
        "                break\n",
        "        except ValueError:\n",
        "            pass\n",
        "        print(\"Invalid model number. Please try again.\")\n",
        "\n",
        "    # Initialize chat session\n",
        "    chat_session = initialize_chat(model)\n",
        "    print(f\"\\nStarting chat with {model}\")\n",
        "    model_info = MODELS[model]\n",
        "\n",
        "    # Display model-specific instructions\n",
        "    if model.startswith(\"dall-e\"):\n",
        "        print(\"\\nDALL-E Image Generation Options:\")\n",
        "        if model == \"dall-e-3\":\n",
        "            print(\"- Available sizes: 1024x1024, 1024x1792, 1792x1024\")\n",
        "            print(\"- Quality options: standard, hd\")\n",
        "            print(\"- One image per request\")\n",
        "            print(\"\\nTip: To prevent prompt enhancement, start with:\")\n",
        "            print('\"I NEED to test how the tool works with extremely simple prompts. DO NOT add any detail, just use it AS-IS:\"')\n",
        "        else:  # dall-e-2\n",
        "            print(\"- Available size: 1024x1024\")\n",
        "            print(\"- Can generate up to 10 images per request\")\n",
        "    else:\n",
        "        if \"context_window\" in model_info:\n",
        "            print(f\"Context Window: {model_info['context_window']} tokens\")\n",
        "            print(f\"Max Output: {model_info['max_output']} tokens\")\n",
        "        if \"description\" in model_info:\n",
        "            print(f\"Description: {model_info['description']}\")\n",
        "\n",
        "    print(\"\\nType 'exit' to end the conversation\")\n",
        "    print(\"Type 'transcript' to view the current transcript\")\n",
        "\n",
        "    # For DALL-E-3, add size and quality options\n",
        "    if model == \"dall-e-3\":\n",
        "        # Size selection\n",
        "        sizes = {\n",
        "            \"1\": \"1024x1024\",\n",
        "            \"2\": \"1024x1792\",\n",
        "            \"3\": \"1792x1024\"\n",
        "        }\n",
        "        print(\"\\nAvailable Image Sizes:\")\n",
        "        for num, size in sizes.items():\n",
        "            print(f\"{num}. {size}\")\n",
        "\n",
        "        while True:\n",
        "            size_choice = input(\"\\nChoose size (number) [default: 1]: \").strip()\n",
        "            if not size_choice:\n",
        "                size_choice = \"1\"\n",
        "            if size_choice in sizes:\n",
        "                size = sizes[size_choice]\n",
        "                break\n",
        "            print(\"Invalid choice. Please try again.\")\n",
        "\n",
        "        # Quality selection\n",
        "        qualities = {\n",
        "            \"1\": \"standard\",\n",
        "            \"2\": \"hd\"\n",
        "        }\n",
        "        print(\"\\nAvailable Quality Options:\")\n",
        "        for num, qual in qualities.items():\n",
        "            print(f\"{num}. {qual}\")\n",
        "\n",
        "        while True:\n",
        "            quality_choice = input(\"\\nChoose quality (number) [default: 1]: \").strip()\n",
        "            if not quality_choice:\n",
        "                quality_choice = \"1\"\n",
        "            if quality_choice in qualities:\n",
        "                quality = qualities[quality_choice]\n",
        "                break\n",
        "            print(\"Invalid choice. Please try again.\")\n",
        "\n",
        "        chat_session[\"dalle_options\"] = {\"size\": size, \"quality\": quality}\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nYou: \").strip()\n",
        "\n",
        "        if user_input.lower() == 'exit':\n",
        "            break\n",
        "        elif user_input.lower() == 'transcript':\n",
        "            print_transcript(chat_session)\n",
        "            continue\n",
        "\n",
        "        if user_input:\n",
        "            response = send_message(chat_session, user_input)\n",
        "            if response:\n",
        "                print(\"\\nAssistant:\", response)\n",
        "\n",
        "    # Print final transcript\n",
        "    print_transcript(chat_session)\n",
        "\n",
        "    # Ask if user wants to export\n",
        "    if input(\"\\nExport transcript to file? (y/n): \").lower() == 'y':\n",
        "        export_transcript(chat_session)"
      ],
      "metadata": {
        "id": "VYsSCtVTEYUU"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Start the interactive chat with a model of your choice**\n",
        "Finally, run this cell to actually start the conversation with a model of your choice. You will first be asked to pick a 'model family (e.g. 4o family, o1 family)' and then the actual model you wish to chat with by typing in a number. Type 'exit' if you wish to end the conversation. You will be given the option to export the conversation transcript in .json at the end.\n",
        "\n",
        "  See the full list of models that OpenAI provides here for more information about the models: https://platform.openai.com/docs/models."
      ],
      "metadata": {
        "id": "giEzfXo-JfsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the interactive chat\n",
        "start_interactive_chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8Nnk68iKtVG",
        "outputId": "8862fa13-47e4-436a-c47f-461324558e88"
      },
      "execution_count": 28,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Available Model Categories:\n",
            "1. GPT-4o Series\n",
            "2. GPT-4o Mini Series\n",
            "3. o1 Series\n",
            "4. DALL-E Series\n",
            "5. GPT-4o Realtime Preview\n",
            "6. GPT-4o Audio Preview\n",
            "7. GPT-4 Turbo and GPT-4\n",
            "8. GPT-3.5 Turbo\n",
            "\n",
            "Choose category (number): 4\n",
            "\n",
            "Available Models in Category:\n",
            "1. dall-e-3\n",
            "   Description: Latest DALL·E model released in Nov 2023\n",
            "2. dall-e-2\n",
            "   Description: Previous DALL·E model with 4x greater resolution than original\n",
            "\n",
            "Choose model number: 1\n",
            "\n",
            "Starting chat with dall-e-3\n",
            "\n",
            "DALL-E Image Generation Options:\n",
            "- Available sizes: 1024x1024, 1024x1792, 1792x1024\n",
            "- Quality options: standard, hd\n",
            "- One image per request\n",
            "\n",
            "Tip: To prevent prompt enhancement, start with:\n",
            "\"I NEED to test how the tool works with extremely simple prompts. DO NOT add any detail, just use it AS-IS:\"\n",
            "\n",
            "Type 'exit' to end the conversation\n",
            "Type 'transcript' to view the current transcript\n",
            "\n",
            "Available Image Sizes:\n",
            "1. 1024x1024\n",
            "2. 1024x1792\n",
            "3. 1792x1024\n",
            "\n",
            "Choose size (number) [default: 1]: 1\n",
            "\n",
            "Available Quality Options:\n",
            "1. standard\n",
            "2. hd\n",
            "\n",
            "Choose quality (number) [default: 1]: 2\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "You: Generate the city of New York with pink and green alien llama spaceship above it. Swigging past the building and to the ship is a heroic red and blue superhero.\n",
            "\n",
            "Assistant: Image generated successfully!\n",
            "\n",
            "Revised prompt: Depict a bustling scene of New York City, known for its towering structures and busy streets. Hovering in the sky above this landscape is a peculiar sight, a spaceship characterized by unfamiliar details, resembling a llama in form, and decked out in curious hues of pink and green. Also in the sky, giving the impression of swaying between the city's skyscrapers and toward the spaceship is a superhero of unknown identity. This figure dons a vibrant costume, predominantly in shades of red and blue, symbolizing their courage and commitment to safeguard the city.\n",
            "\n",
            "Image URLs:\n",
            "https://oaidalleapiprodscus.blob.core.windows.net/private/org-tHVp2E07XGFWyHrPcjAtsqyD/user-5kbXwVfAHrzslcWWm90VlxX0/img-v2OuOS8Pl6iGgdHm6DqdMkR0.png?st=2025-01-21T05%3A21%3A42Z&se=2025-01-21T07%3A21%3A42Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-01-21T01%3A14%3A48Z&ske=2025-01-22T01%3A14%3A48Z&sks=b&skv=2024-08-04&sig=nWTkbbAxKNGxC/tc7uC6r5URzNgarJfIPAXVSfFYB4k%3D\n",
            "\n",
            "You: Remove the spaceship and give the red and blue masked hero web slingers. He swings across the city of new york.\n",
            "\n",
            "Assistant: Image generated successfully!\n",
            "\n",
            "Revised prompt: A heroic figure, clad in a mask of red and blue, equipped with devices on his wrists that mimic the ability of a spider to spin webs. Swiftly and gracefully, he swings across the bustling city of New York, amidst towering skyscrapers and hustling streets. The glimmering cityscape provides a dynamic backdrop as starlight begins to encroach upon the twilight hour. There is no sight of a spaceship in the frame, focusing solely on this masked vigilante and his urban playground.\n",
            "\n",
            "Image URLs:\n",
            "https://oaidalleapiprodscus.blob.core.windows.net/private/org-tHVp2E07XGFWyHrPcjAtsqyD/user-5kbXwVfAHrzslcWWm90VlxX0/img-Hxeg6x9IcyKLFLLRQP5SGGkD.png?st=2025-01-21T05%3A22%3A10Z&se=2025-01-21T07%3A22%3A10Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-01-21T01%3A35%3A10Z&ske=2025-01-22T01%3A35%3A10Z&sks=b&skv=2024-08-04&sig=CtCLIXvclmtpeYyipoV/ilk8uG6YmaG9fEWuMY5%2BcDk%3D\n",
            "\n",
            "You: exit\n",
            "\n",
            "Chat Transcript - Started at 2025-01-21 06:21:12\n",
            "Model: dall-e-3\n",
            "Description: Latest DALL·E model released in Nov 2023\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "USER:\n",
            "Generate the city of New York with pink and green alien llama spaceship above it. Swigging past the building and to the ship is a heroic red and blue superhero.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "ASSISTANT:\n",
            "Image generated successfully!\n",
            "\n",
            "Revised prompt: Depict a bustling scene of New York City, known for its towering structures and busy streets. Hovering in the sky above this landscape is a peculiar sight, a spaceship characterized by unfamiliar details, resembling a llama in form, and decked out in curious hues of pink and green. Also in the sky, giving the impression of swaying between the city's skyscrapers and toward the spaceship is a superhero of unknown identity. This figure dons a vibrant costume, predominantly in shades of red and blue, symbolizing their courage and commitment to safeguard the city.\n",
            "\n",
            "Image URLs:\n",
            "https://oaidalleapiprodscus.blob.core.windows.net/private/org-tHVp2E07XGFWyHrPcjAtsqyD/user-5kbXwVfAHrzslcWWm90VlxX0/img-v2OuOS8Pl6iGgdHm6DqdMkR0.png?st=2025-01-21T05%3A21%3A42Z&se=2025-01-21T07%3A21%3A42Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-01-21T01%3A14%3A48Z&ske=2025-01-22T01%3A14%3A48Z&sks=b&skv=2024-08-04&sig=nWTkbbAxKNGxC/tc7uC6r5URzNgarJfIPAXVSfFYB4k%3D\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "USER:\n",
            "Remove the spaceship and give the red and blue masked hero web slingers. He swings across the city of new york.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "ASSISTANT:\n",
            "Image generated successfully!\n",
            "\n",
            "Revised prompt: A heroic figure, clad in a mask of red and blue, equipped with devices on his wrists that mimic the ability of a spider to spin webs. Swiftly and gracefully, he swings across the bustling city of New York, amidst towering skyscrapers and hustling streets. The glimmering cityscape provides a dynamic backdrop as starlight begins to encroach upon the twilight hour. There is no sight of a spaceship in the frame, focusing solely on this masked vigilante and his urban playground.\n",
            "\n",
            "Image URLs:\n",
            "https://oaidalleapiprodscus.blob.core.windows.net/private/org-tHVp2E07XGFWyHrPcjAtsqyD/user-5kbXwVfAHrzslcWWm90VlxX0/img-Hxeg6x9IcyKLFLLRQP5SGGkD.png?st=2025-01-21T05%3A22%3A10Z&se=2025-01-21T07%3A22%3A10Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-01-21T01%3A35%3A10Z&ske=2025-01-22T01%3A35%3A10Z&sks=b&skv=2024-08-04&sig=CtCLIXvclmtpeYyipoV/ilk8uG6YmaG9fEWuMY5%2BcDk%3D\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Export transcript to file? (y/n): n\n"
          ]
        }
      ]
    }
  ]
}